---
title: "Supplementary Information: How's the Air Out There? Using a National Air Quality Database to Introduce
  First Year Students to the Fundamentals of Data Analysis"
author: "David Hall and Jessica D'eon (Corresponding Author)"
date: "19/03/2021"
output:
  bookdown::word_document2: default
  html_document:
    df_print: paged
  bookdown::html_document2: default
bibliography: references.bib
csl: american-chemical-society.csl
---

```{r setup, include=FALSE}

library(lubridate)
library(tidyverse)
library(cowplot)
library(scales)

knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, error = FALSE)
```



## Leveraging R to automate and expand the lab

We made prodigeous use of *R* and various associated frameworks to greatly facilitate many aspects of this lab. Greater details and source code can be found in the [Supporting Information], but a brief discussion is warranted, if anything else, to encourage readers to harness *R* (or similar data science languages) to both simplify their own workload, while expanding course content.

Firstly, the generation of the 7-day datasets. The NAPS program compiles all hourly measurements of a single pollutant, across all monitoring stations (n~station~\>150), for a single calendar year in one large `.csv` file. These expansive datasets are organized in a matrix style with columns for hour of the day (n~columns~ \> 24), and rows for days of the year at each station (n~rows~ \> (n~station~ $\times$ 365 days/year)). For first-year students, the task of accessing these datasets to subset their working data would be immediately overwhelming and tedious as they often exceed tens of thousands of rows and contain a number of information not necessary for their work. To counteract this, we used *R* to combine O~3~ and NO~2~ measurements (stored in separate `.csv` files), match them by NAPS station, and remove much of the ultimately unnecessary information (i.e. bilingual headers). The merged data is then transformed from the 'wide' matrix style to the 'long' columnar format (each row is [O~3~] and [NO~2~] per hour) so data is easier to manipulate in Excel. Then, using *R*, we generate a specific number of student datasets using a 7-day moving window of the year-long data. I.e. dataset 1 is Jan. 1st to the 7th, dataset 2 is Jan 2nd to 8th, etc, with complimentary summer data sets taken from July 1st onward. The rolling window ensure every student can be assigned a unique dataset, while largely looking at similar data. We also randomly insert a '-999' missing value into each data set ensuring students will encounter it during their analysis. Each data set is then automatically saved as a .csv file.

Secondly, we wrote an *R markdown* script that generates a PDF with the analysis results of every generated data sets. The answer sheet analysis mirrors the one students carry out, providing TAs with an actual analysis of every dataset assigned to their students allowing them to quickly check each students submission, and relieving them of the burden of verifying each dataset.

Thirdly, we wrote an interactive application using *R* and *Shiny*. This was created in-house specifically for this laboratory exercise. Thanks to the this, we were able to expand students working data from that directly provided to them, to the entire NAPS dataset. This would be impossible otherwise as students would not have the ability or time to explore the larger NAPS data in any practical manner given the tools we can provide to them. See the [Supplementary Information] for more details on the app, and how you can recycle our code to create your own version of the application if you want to run this lab.

Lastly, our code can accept any standard formatted O~3~ and NO~2~ NAPS datasets, and instructors can readily select the NAPS station, the number of datasets, the overlap between datasets, etc. so course material can easily be updated for each iteration of the lab, or between different lab sections. See the [Supplementary Information] for the source-code for instructions on generating datasets.


## App Metrics

## Notes on the Air Quality Shiny App

While the idea of creating an app for a specific lab exercise may seem daunting, the flexibility and support of the Shiny framework greatly relieve one from the minutia of app development, allowing them to focus on how best to present their data to the target audience. The version of our app presented to students in the Winter 2021 term is available for viewing here: <https://davidrosshall.shinyapps.io/AirQualityApp/>. We have also provided the complete source code and example datasets on Github: <https://github.com/DavidRossHall/AirQualityApp>, and as a `.zip` in the supplementary information. We have provided instructions on running the app, and more importantly, we strongly encourage interested parties to modify the app to best suit their pedagogical needs.

With any software, we needed to evaluate student experiences with our Air Quality app. However, we did not explicitly probe students for direct feedback, nor did we use capitalize on the abilities of Google analytics (although that is an option when creating Shiny Apps) to track student usage of the app. Instead, we inferred app usage by tracking the number of accessions (i.e. times the app was used). As shown in Figure \@ref(fig:app-connections), despite publishing the course material more than a week prior to the synchronous lab sessions, students did not access the app in any meaningful numbers. Usage did increase after the synchronous session, where students were explicitly instructed to work on their Lab 1 reports, of which two questions explicitly instructed students to access the Air Quality App. Predictably, app usage was highest immediately preceding the due date for the Lab 1 report; the implications of this are discussed below. As best we can tell, usage/interaction with the Air Quality app lasted on average 25 minutes, a respectable time given the brevity of the prompting questions, and the richness of the dataset.

A brief comment for the unaware: running a Shiny app requires server side computing. In other words, as students select data to plot, a dedicated server must perform the necessary computations. While these requirements are not egregious, they are still to be considered if you plan on hosting your own instance of the app. While Shiny provides instructions and software for running the app on premise, we opted to host our app on the Shiny server cloud. As mentioned above, many students 'flashed' the app as the deadline approached. Without adequate server space, this could increase load times, decrease responsiveness, and possibly crash the app. The free 15hrs/month of server time (time it takes to run the app) provided by Shiny is unsuitable for the anticipated server loads once the App was released for all of the CHM135 lab sections. Consequently, for the 2021 Winter Term, there were slightly over ??? students, so we payed the *Standard* hosting package costing \$99USD/month for 2000 hrs of server time. This was admittedly an excess of server time, but we chose to play it safe. Likewise, as the Air Quality App was only used during the first lab, we only needed to pay for one month of server time to account for all the lab sections. As we continue to experiment with creating purpose built apps for individual labs/courses, we will attempt to transition to local hosting.

```{r app-connections_functions}

conMetrics.cleanup <- function(file, startDate, endDate){
  
  # Collecting data spanning lab period
  df <- read.csv(file) %>%
    mutate(date=as.POSIXct(as.numeric(as.character(timestamp)),origin="1970-01-01")) %>% 
    mutate(connect_count = as.numeric(connect_count)) %>%
    mutate(connect_procs = as.numeric(connect_procs)) %>%
    select(-timestamp) %>% 
    filter(date > ymd_hms(startDate) & date < ymd_hms(endDate)) %>%
   arrange(date) %>% 
    mutate(
     n_count=cumsum(connect_count),
      n_procs=cumsum(connect_procs),
      new_connect=case_when(
        connect_count>lag(connect_count,1) ~ connect_count-lag(connect_count,1),
        TRUE ~ 0),
      n_connect=cumsum(new_connect) # approximate
    ) %>% 
    filter(n_count>0)
  
  df
}

# plot of cumulative connections 

cumsum.plot <- function(df){
  
  df %>%
    select(n_connect, date) %>% 
    gather(key="key", value="value", -date) %>%
    ggplot() +
    labs(x="Date", y="Cumulative\nConnections") +
    geom_line(aes(x=date, y=value)) +
    scale_x_datetime(date_breaks = "7 day", date_labels = "%a \n%b %d") +
    theme_classic()
  
  }


# col plot of connections per day

daycount.dat <- function(df){
  
  df %>%
    mutate(date2 = as.Date(date)) %>%
    group_by(date2) %>%
    summarize(size = max(n_connect)) %>%
    mutate(dayCount = size - lag(size)) 
}



```

```{r app-connections, fig.width = 8, fig.height = 5, fig.fullwidth = TRUE, fig.cap="Connections to Air Quality app per day (top) and cummulative connections over time (bottom) for the Winter and Fall 2021 sessions. Red bars denote days when the app crashed and required resetting and expansion of number of allowable connections. Note discrepency of y-axis between Winter and Fall 2021 sessions."}

# Jan2021 Data 

fileJan2021 <- "dataForPaper/container_status_Practical1.csv"

Jan2021<- conMetrics.cleanup(file = fileJan2021, 
                                startDate = "2021-01-10 00:00:00", 
                                endDate = "2021-01-31 00:00:00" )

# Fall2021 Data

fileFall2021 <- "./dataForPaper/Fall2021.csv"
Fall2021 <- conMetrics.cleanup(file = fileFall2021, 
                                startDate = "2021-09-17 00:00:00", 
                                endDate = "2021-10-18 00:00:00")

# Plotting cummulative and daily connections
cumsumJan <- cumsum.plot(Jan2021) +
  geom_vline(xintercept = ymd_hms("2021-01-10 18:00:00"), color = "black", lwd = 1) + #lab published on quercus
  geom_vline(xintercept = ymd_hms("2021-01-21 09:00:00"), color = "black", lwd = 1) + #Sync sessions starts
  geom_vline(xintercept = ymd_hms("2021-01-27 22:00:00"), color = "black", lwd = 1) + #due date 
  annotate("text", x = ymd_hms("2021-01-11 00:00:00"), y = 250, label = "Released", hjust = 0, size=3) +
  annotate("text", x = ymd_hms("2021-01-21 00:00:00"), y = 125, label = "Start sync. \nsessions", hjust = 1, size=3) +
  annotate("text", x = ymd_hms("2021-01-27 23:59:00"), y = 125, label = "Due date", hjust = 0, size=3) 

daycountJan<- daycount.dat(Jan2021) %>%
    ggplot(., aes(x = date2, y = dayCount)) +
    geom_col(fill="gray") + 
    theme(axis.title=element_blank()) +
    labs(y = "Connections\nper day") +
    theme_classic() +
    theme(axis.line.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank()) +
  labs(subtitle = "Winter 2021")

    
cumsumFall <- cumsum.plot(Fall2021) +
    geom_vline(xintercept = ymd_hms("2021-10-16 00:00:00"), color = "black", lwd = 1) + #due date 
    annotate("text", x = ymd_hms("2021-10-16 00:00:00"), y = 1000, label = "End of\nExp. 1", hjust = 1.2, size=3) +
  labs(y = "") 

daycountFall <- daycount.dat(Fall2021) %>%
    mutate(appStatus = case_when(as.Date(date2) == as.Date("2021-10-04") ~ "crashed",
                               as.Date(date2) == as.Date("2021-09-29") ~ "crashed",
                               TRUE ~ "ok")) %>%
    ggplot(., aes(x = date2, y = dayCount, fill = appStatus)) +
    geom_col(show.legend=FALSE) + 
    scale_fill_manual(values=c("red","grey70")) +
   theme_classic() +
   theme(axis.line.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.x=element_blank(),
        panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank(),
        axis.title.y=element_blank(),legend.position="none") +
     labs(y = "", 
          subtitle = "Fall 2021")

cowplot::plot_grid(daycountJan, daycountFall, cumsumJan, cumsumFall, align = "v" )
```

# Complete student feedback 

```{r lab1-Feedback, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

lab1Feedback <- read_csv("dataForPaper/CompleteLab1SurveyData.csv") %>%
  filter(QuestionID == 1) %>%
  select(c("Question", "Answer", "time", "count"))


Q1 <- paste( "Student responses to: ", unique(lab1Feedback$Question))

knitr::kable(select(lab1Feedback, -Question), caption = (Q1))

```

```{r chm135-Feedback, echo = FALSE, error = FALSE, message = FALSE, warning = FALSE}

chm135Feedback <- read_csv("dataForPaper/CompleteLab1SurveyData.csv") %>%
  filter(QuestionID == 5) %>%
  select(c("Question", "Answer", "time", "count"))


Q5 <- paste( "Student responses to: ", unique(lab1Feedback$Question))

knitr::kable(select(chm135Feedback, -Question), caption = (Q5))

```