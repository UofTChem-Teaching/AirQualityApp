---
title: How's the Air Out There? Using a National Air Quality Database to Introduce
  First Year Students to the Fundamentals of Data Analysis
author: "David Hall and Jessica D'eon (Corresponding Author)"
date: "19/03/2021"
output:
  bookdown::word_document2: default
  html_document:
    df_print: paged
  bookdown::html_document2: default
bibliography: references.bib
csl: american-chemical-society.csl
---

```{r setup, include=FALSE}

library(tidyverse)
library(scales)     # for text wrap on x-axis
library(lubridate)  # for app metrics date conversions
library(cowplot)    # for app metrics multiplot

library(ggpubr)
library(ggpmisc)
library(ggExtra)
library(openxlsx)
library(RcppRoll)

library(gridExtra)
library(grid)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, error = FALSE)
```

# Abstract

# Introduction

Whether we like it or not we are living in an age of data, and the world of chemistry is no exception. From big-data atmospheric chemistry in climate-change models[@thefutu2016] to machine-learning organic synthesis[@dealmeida2019], every domain of chemistry is increasingly relying upon data-driven science. In response to this trend, undergraduate chemistry curricula need to adapt to better equip and prepare the next-generation of chemists with the necessary skills and knowledge to navigate an increasingly data-centric world. As we ourselves work on new undergraduate teaching material, we notice that an oft-overlooked aspect in how data analysis is presently tough is how exactly data (measurements, signals, etc.) is transformed into information (trends, correlation) and finally into knowledge. The explicit teaching of these concepts is often neglected in current teaching labs, resulting in increasing student frustration. Motivated by this, and the need to transfer to a virtual laboratory environment as a result of Covid-19 social distancing restrictions, we sough to develop a new, remote learning compatible, experiment.

An obvious assumption of teaching data science is that students will eventually analyze *real* data, which is often permeated with outliers and the fingerprints of gross experimental errors. The data that undergraduate chemist will collect in their academic careers is no different. However, acquiring sufficient data for analysis if often stressful for undergraduate students in a teaching laboratory setting due to time- and equipment-constraints. As well, with the Covid-19 restrictions students were unable to attend labs, and hence unable to produce any of their own data. We saw this as an opportunity to integrate actual measurements from published data repositories. A choice example is the air quality data from Environment and Climate Change Canada's national Air Pollution Surveillance Programs (NAPS). There is no shortage of data to be analyzed, because the NAPS program has been conducting hourly measurements across Canada since 1975 of several major atmospheric.

Prominent atmospheric pollutants are structurally simple, and undergo reaction schemes comparable to those covered in introductory chemistry lectures. Ozone (O~3~) and nitrogen dioxide (NO~2~) are two choice candidates for analysis by undergraduate students. They are structurally simple molecules, and undergo reaction schemes comparable to those covered in introductory chemistry lectures (see reactions \@ref(eq:O3-1), \@ref(eq:O3-2), and \@ref(eq:O3-3)). Notable of these compounds is their interdependent diurnal cycles. The relationship between O~3~ and NO~2~ is so intimate, the term "odd-oxygen" (O~x~) is used to express the sum of these two compounds (see reaction \@ref(eq:odd-ox), and ref. [@Kley1994]), although the relationship between O~3~ and NO~2~ can vary with environmental and anthropogenic influences including temperature, sunlight, and motor vehicle emissions.

$$
\begin{align}
  NO_2 + photon &\rightarrow NO + O (\#eq:O3-1) \\
  O + O_2 &\rightarrow O_3 (\#eq:O3-2)  \\
  O_3 + NO &\rightarrow O_2 + NO_2 (\#eq:O3-3)\\
  [O_3] + [NO_2] &= [O_x] (\#eq:odd-ox)
\end{align}
$$

While students investigate seasonal differences in the relationship between atmospheric O~3~ and NO~2~ from subsets of the NAPS dataset, our Air Quality lab explicitly introduces data analysis work-flows. As students work through the instructions they are encouraged to generate hypothesis through probing questions, which they further investigate across the entire NAPS dataset through a new interactive application we developed. As the entire lab uses previously acquired data, and the ubiquitous Microsoft Excel software package, students were able to explore real data from home, and complying with Covid-19 restrictions.

# Experimental Overview And Pedagogical Goals

This 3 hr data-analysis laboratory exercise uses publicly available data and open-source code (described in the [Supplementary information]), and has been run successfully in the one-semester "CHM135: Chemistry: Physical Principles" undergraduate general chemistry at the University of Toronto since Summer 2020. This course is most often conducted in the first-term of the first-year of life-sciences/chemistry undergraduate students. As our *Air Quality Lab* being the first-lab of five, it is designed as much as an tutorial on data-analysis and Microsoft excel as it is to explore atmospheric chemistry. The lab is divided into three parts: the prelab, data analysis in Excel, and data exploration & hypothesis generation.

The prelab follows a traditional approach, and is written to situate students in the relevant chemistry for the upcoming analysis. Specifically for this lab we create explanatory videos and material introducing gas phase chemistry, and relating the lab content to concurrent lecture material of gas phase chemistry (i.e. ideal gas law).

In the data analysis portion of the lab students are randomly assigned two datasets. Each dataset is a 7-day snapshots of hourly O~3~ and NO~2~ measurements taken from the NAPS program. The datasets are all from the same NAPS surveillance station for a given year. For our purposes we chose a different downtown Toronto NAPS station for each successive iteration of the lab. The two datasets correspond to 7-days in the winter and 7-days in the summer, and were generated from original NAPS data using R as described below. Alongside their data, students are provided with a written handbook detailing the necessary Excel operations, and an synchronous online session with their TA. Working through the lab exercises students are explicitly taught data analysis workflows, modeled after that recommended by Hadley and Grolemund[@Wickham2017]:

1.  *Importing* their assigned comma serrated values (.csv) data sets into Excel.
2.  *Tidying* their data and setting up their worksheets. This step consists of formatting cells to properly display values and handling missing data. Specifically for this lab, the NAPS dataset stores missing values as '-999', which can be erroneously interpreted literally by Excel.
3.  *Visualizing* their data by creating a time-series plot of time vs. concentration of each pollutant, see Figure \@ref(fig:example-plots).
4.  *Transforming* their data using mathematical operators in Excel to calculate total oxidant and adding it to their time-series plot as well as calculating 8hr moving averages.
5.  *Modeling* a linear relationship between [O~3~] and [NO~2~] to qualitatively assess the negative relationship between these two contaminants.
6.  *Communicating* and exploring their results through a series of accompanying questions.

```{r example-plots, fig.margin=TRUE, echo = FALSE, message = FALSE, warning=FALSE, fig.height = 7, fig.cap = "Example of plots students are expected to create. (A) time-series of pollutants across 7 winter days. (B) Correlation plot of O3 and NO2 concentrations with linear regression in the winter and (C) summer data sets. (D) Example plot if a '-999' value wasn't removed."}

data <- read_csv("dataForPaper/Toronto_60410_2018_Day10to16.csv",)

dataSummer <- read_csv("dataForPaper/Toronto_60410_2018_Day189to195.csv")

data <- data %>%
  mutate(time = convertToDateTime(data$Date, origin = "1900-01-01")) %>%
  filter(O3 != -999) %>%
  filter(NO2 != -999) %>%
  mutate(OX = NO2 + O3)

### Making data tidyR friendly --------------------------------------------------
dataCol <- data %>%
  select(-c("Date")) %>%
  pivot_longer(-time, names_to = "pollutant", values_to = "concentration")


dataSummer <- dataSummer %>%
  mutate(time = convertToDateTime(dataSummer$Date, origin = "1900-01-01")) %>%
  filter(O3 != -999) %>%
  filter(NO2 != -999) %>%
  mutate(OX = NO2 + O3)

### Making data tidyR friendly --------------------------------------------------
dataSummerCol <- dataSummer %>%
  select(-c("Date")) %>%
  pivot_longer(-time, names_to = "pollutant", values_to = "concentration")

### Time series ----------------
a <- ggplot(data = dataCol, aes(x = time, y = concentration, color = pollutant)) +
  geom_line(size = 1) +
  theme_classic() +
   theme(text = element_text(size = 12),
         legend.position = "right") +
  ylab(bquote('Conc., ppb')) +
  xlab(bquote('Time')) 

### Correlation plot with Linear regression and equation -------------------------

formula <- y ~ x ### Need to keep this so LM regression appears on plot

b <- ggplot(data = data, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_smooth(method = "lm", formula = formula, se = FALSE) +
    stat_poly_eq(aes(label =  paste(stat(rr.label), sep = "*\", \"*")),
               formula = formula, rr.digits = 4 , parse = TRUE, label.y = 0.25, label.x = 0.95, size = 4)+
  annotate("text", x =35, y = 33, label = "winter")

c <- ggplot(data = dataSummer, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(0, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_smooth(method = "lm", formula = formula, se = FALSE) +
    stat_poly_eq(aes(label =  paste(stat(rr.label), sep = "*\", \"*")),
               formula = formula, rr.digits = 4 , parse = TRUE, label.y = 0.25, label.x = 0.95, size = 4) +
  annotate("text", x =36, y = 33, label = "summer")

# -999 value for example

dataSummer[2,3] = -999
  
d <- ggplot(data = dataSummer, aes(x = NO2, y = O3)) +
  geom_point(size = 0.5) + 
  scale_x_continuous(expand = c(0, 0), limits = c(0, 45)) + 
  scale_y_continuous(expand = c(0, 0), limits = c(-1050, 70)) +
  theme_classic() +
  theme(text = element_text(size = 12))+
  xlab(bquote('Conc.' ~NO[2]~', ppb')) +
  ylab(bquote('Conc.' ~O[3]~', ppb')) +
  geom_segment(aes(x = 22, y = -600, xend = 16.5, yend = -950),
                  arrow = arrow(length = unit(0.5, "cm"))) +
  annotate("text", x = 32, y = -550, label = "error from analysis")

gt <- arrangeGrob(a, b, c, d,                               
             ncol = 2, nrow = 2)
# Add labels to the arranged plots
p <- as_ggplot(gt) +                                # transform to a ggplot
  draw_plot_label(label = c("A", "B", "C", "D"), size = 16,
                  x = c(0, 0.5, 0, 0.5), y = c(1, 1, 0.5, 0.5)) # Add labels
p

```

The last step in this workflow is expanded in the final part of the lab where students compare their results to the complete NAPS dataset from which their assigned datasets originated from. Here they are encouraged to generate hypotheses based on their own data, and their *a priori* chemical knowledge introduced in the prelab. To this end, we created an interactive online application that students visit using *R* and *Shiny*[@chang2020]. This application consist of an interactive map showing the location, and local population, of every NAPS surveillance station contained in the dataset. Students can then select any station and time-span, and a time-series and correlation plot, similar to the ones they created themselves, are automatically generated. This allows them to rapidly compare their data to any number of stations, simultaneously relieving them of the burden of repetitive and tedious data analysis while facilitating hypothesis generation and data exploration. Accompanying questions prompt students to explore and reason differences in O~3~ and NO~2~ correlation between urban and rural areas, as well as between winter and summer datasets. See the [Supporting Information] or <https://davidrosshall.shinyapps.io/AirQualityApp/> for details on the application.



# Results and Pedagogical Outcomes

```{r lab1survey, fig.width=7, fig.height = 4}

# importing Lab 1 survey data
lab1Survey <- read_csv("dataForPaper/CompleteLab1SurveyData.csv")


# survey plot function
surveyPlot <- function(q, df, axisWidth = 30, subWidth = 60, terms){
  
  df <- subset(df, QuestionID == q & time %in% terms) 
  
  question <- str_wrap(unique(df$Question), subWidth)

  ggplot(df, aes(x = Answer, 
           y = per, 
           fill = time)) +
  geom_bar(stat = "identity", 
           position = "dodge")  +
    labs(subtitle = question) +
    ylab("Percentage") +
    xlab("") +
    expand_limits(y = 20) +
    coord_flip() +
    ylim(0,100) +
    #scale_y_continuous(minor_breaks = seq(0, 100, 4)) +
    scale_x_discrete(labels = label_wrap(axisWidth)) +
    theme_light() +
    theme(
      panel.grid.major.y = element_blank(),
      panel.border = element_blank(),
      axis.ticks.y = element_blank(),
      text = element_text(size = 8)
      )
    
   
}

# plots per survey question
f1 <- surveyPlot(q = 2, 
                 df = lab1Survey, 
                 terms = c("Nov. 2021", "Apr. 2021"),
                 subWidth = 30,
                 axisWidth = 60)

f2 <- surveyPlot(q = 3, 
                 df = lab1Survey, 
                 terms = c("Nov. 2021", "Apr. 2021"), 
                 subWidth = 30, 
                 axisWidth = 30)

f3 <- surveyPlot(q = 4, 
                 df = lab1Survey, 
                 terms = c("Nov. 2021", "Apr. 2021"),
                 subWidth =  30,
                 axisWidth = 30)

p <- ggpubr::ggarrange(f2, f3, f1, 
                       ncol = 2, nrow = 2,  
                       common.legend = TRUE, 
                       legend = "bottom"
                   )

p

```

Based on the our surveys and feedback from students, the majority of students responded positively to the new learning experience. From our survey of students in the most recent iteration of the Air Quality Lab in the winter 2021 term (n = ??), 68% of respondents stated they felt the Excel component of Lab 1 significantly helped them complete the other CHM135 labs more efficiently. (Figure \@ref(fig:plot-survey-results)). This is a welcomed improvement as students frequently complain about the time commitment required for the CHM135 lab component. Students also feel more confident with regards to overall interpretation of data (plots, trendlines) as well as towards their use of Excel in general (57%, and 64%, respectively) (Figure \@ref(fig:plot-survey-results)).

Included in the survey was the option for students to provide any additional feedback on the lab. Students expressed both positive and negative feedback to the Air Quality lab (See [Supporting Information] Table \@ref(fig:lab1-Feedback) for complete feedback). Students appreciated the introduction to Excel, the practical usefulness of the incorporated material, and the opportunity to analyze real world data offering a glimpse into environmental chemistry. However, students were also critical of how the material was implemented. Some experienced trouble with inconsistencies between the Excel instructions, and their version of Excel (although all UofT students are provided with free access to the latest version of Excel, with guidance/links provided in the aforementioned lab instructions document). Likewise, many students felt the Excel component should have been explicitly tough during the synchronous session, rather than those sessions focusing explicitly on lab material (i.e. data analysis vs. Excel operations). The CHM135 contains hundreds of students, many of whom have prior experience with Excel. Consequently, we opted not to directly teach students the basic workings of Excel in the synchronous session, as this would have bored many of students. Going forward, we feel that incorporating optional Excel help-sessions specifically to assist students with this component of the lab.

A major addition to this lab is the development of the Air Quality App using Shiny. We strongly believe this was a great component, as the functionality of the app greatly reduces the 'friction' allowing students to readily explore the larger NAPS dataset without burdening them with lengthy data prep/analysis. Furthermore, as students explore the larger NAPS dataset they see data 'unknown' to the instructors. As the data is from real and complex environments, students often find data that *does not* support their hypothesis or that contradicts the pre-lab information! A stark contrast to the traditional undergraduate lab which is tailored so that students produce predictable results. The framework of the latter inadvertently teaches students to fit their data to the theory, in opposition to actual research where the data drives the theories. In other words, students are excited to make real connections and real discoveries with real data rather then going through the often tedious motions of analyzing pre-screened or synthetic data.

# Conclusions

We sought to create a new introductory lab experiment to expressly teach incoming students fundamental components of data analysis/science as well as practical instructions on how to use Microsoft Excel to for a solid foundation for future courses. To this end, we leveraged the R computer language for the automated, and scalable, generation of unique data from real world atmospheric measurements from the NAPS that served as the basis of our introductory data science lab. Alongside written instructions on how to use Excel, we developed an online interactive App, allowing students to readily explore the entire NAPS dataset to compliment their individual analyses. Our efforts were rewarded with students being better equipped to tackle subsequent data analysis challenges in the following introductory chemistry labs, in addition to arming them with skills they will assuredly make use of outside of the first-year chemistry laboratory.

# Supporting Information


# Author Information

David Hall, Department of Chemistry and School of the Environment, University of Toronto.

Jessica D'eon, Department of Chemistry and School of the Environment, University of Toronto.

# Acknowledgements

# References
